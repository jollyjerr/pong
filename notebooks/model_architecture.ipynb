{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "a09b2709",
      "metadata": {},
      "source": [
        "# Pong - Model Architecture"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8f2e81ca",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The autoreload extension is already loaded. To reload it, use:\n",
            "  %reload_ext autoreload\n",
            "adding /Users/jollyjerr/code/@school/pong/src to path\n"
          ]
        }
      ],
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "\n",
        "import sys\n",
        "import os\n",
        "\n",
        "cwd = os.getcwd()\n",
        "src_path = os.path.join(cwd, 'src')\n",
        "\n",
        "print(f'adding {src_path} to path')\n",
        "\n",
        "if src_path not in sys.path:\n",
        "    sys.path.append(src_path)\n",
        "\n",
        "import gymnasium as gym\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import random\n",
        "import itertools\n",
        "import ale_py\n",
        "from collections import deque\n",
        "import cv2\n",
        "import time\n",
        "\n",
        "from dqn_a import DQN\n",
        "\n",
        "gym.register_envs(ale_py)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "id": "fdc75f50",
      "metadata": {},
      "outputs": [],
      "source": [
        "def preprocess(obs):\n",
        "    gray = cv2.cvtColor(obs, cv2.COLOR_RGB2GRAY)\n",
        "    resized = cv2.resize(gray, (64, 64), interpolation=cv2.INTER_AREA)\n",
        "    return resized / 255.0\n",
        "\n",
        "def epsilon_by_frame(frame_idx, eps_start, eps_end, eps_decay):\n",
        "    return max(eps_end, eps_start - (eps_start - eps_end) * frame_idx / eps_decay)\n",
        "\n",
        "def sample_batch(buffer, batch_size, device):\n",
        "    batch = random.sample(buffer, batch_size)\n",
        "    states, actions, rewards, next_states, dones = zip(*batch)\n",
        "\n",
        "    return (\n",
        "        torch.tensor(np.array(states), dtype=torch.float32).to(device),\n",
        "        torch.tensor(np.array(actions), dtype=torch.int64).to(device),\n",
        "        torch.tensor(np.array(rewards), dtype=torch.float32).to(device),\n",
        "        torch.tensor(np.array(next_states), dtype=torch.float32).to(device),\n",
        "        torch.tensor(np.array(dones), dtype=torch.float32).to(device)\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "id": "dc1e5e1e",
      "metadata": {},
      "outputs": [],
      "source": [
        "grid = {\n",
        "    \"learning_rate\": [1e-4, 5e-4],\n",
        "    \"fc_units\": [256, 512],\n",
        "    \"gamma\": [0.95, 0.99]\n",
        "}\n",
        "\n",
        "param_combos = list(itertools.product(*grid.values()))\n",
        "\n",
        "conv_layers = [\n",
        "    {\"out_channels\": 32, \"kernel_size\": 5, \"stride\": 2},  # (32, 30, 30)\n",
        "    {\"out_channels\": 64, \"kernel_size\": 4, \"stride\": 2},  # (64, 14, 14)\n",
        "    {\"out_channels\": 64, \"kernel_size\": 3, \"stride\": 2},  # (64, 6, 6)\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "id": "db52428c",
      "metadata": {},
      "outputs": [],
      "source": [
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "REPLAY_BUFFER_SIZE = 100_000\n",
        "BATCH_SIZE = 32\n",
        "TARGET_UPDATE_FREQ = 1000\n",
        "EPS_START = 1.0\n",
        "EPS_END = 0.1\n",
        "EPS_DECAY = 1e6\n",
        "MAX_FRAMES = 1000 # 200_000\n",
        "STACK_SIZE = 4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "id": "f36d65e2",
      "metadata": {},
      "outputs": [],
      "source": [
        "env = gym.make('ALE/Pong-v5', render_mode=None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "id": "bdb7e108",
      "metadata": {},
      "outputs": [],
      "source": [
        "def run_training(hparams):\n",
        "    learning_rate, fc_units, gamma = hparams\n",
        "    print(f\"\\n=== Training with LR={learning_rate}, FC={fc_units}, Gamma={gamma} ===\")\n",
        "\n",
        "    env = gym.make(\"ALE/Pong-v5\", render_mode=None)\n",
        "    n_actions = env.action_space.n\n",
        "    obs_shape = (STACK_SIZE, 64, 64)\n",
        "\n",
        "    policy_net = DQN(obs_shape, n_actions, fc_units=fc_units, conv_layers=conv_layers).to(DEVICE)\n",
        "    target_net = DQN(obs_shape, n_actions, fc_units=fc_units, conv_layers=conv_layers).to(DEVICE)\n",
        "    target_net.load_state_dict(policy_net.state_dict())\n",
        "    target_net.eval()\n",
        "\n",
        "    optimizer = optim.Adam(policy_net.parameters(), lr=learning_rate)\n",
        "    replay_buffer = deque(maxlen=REPLAY_BUFFER_SIZE)\n",
        "\n",
        "    state = preprocess(env.reset()[0])\n",
        "    state_stack = np.stack([state] * STACK_SIZE, axis=0)\n",
        "    episode_reward = 0\n",
        "    episode_rewards = []\n",
        "\n",
        "    frame_idx = 0\n",
        "\n",
        "    while frame_idx < MAX_FRAMES:\n",
        "        epsilon = epsilon_by_frame(frame_idx, EPS_START, EPS_END, EPS_DECAY)\n",
        "\n",
        "        if random.random() < epsilon:\n",
        "            action = env.action_space.sample()\n",
        "        else:\n",
        "            with torch.no_grad():\n",
        "                state_tensor = torch.tensor([state_stack], dtype=torch.float32).to(DEVICE)\n",
        "                q_values = policy_net(state_tensor)\n",
        "                action = q_values.argmax(dim=1).item()\n",
        "\n",
        "        next_obs, reward, terminated, truncated, _ = env.step(action)\n",
        "        done = terminated or truncated\n",
        "        next_frame = preprocess(next_obs)\n",
        "        next_state_stack = np.roll(state_stack, -1, axis=0)\n",
        "        next_state_stack[-1] = next_frame\n",
        "\n",
        "        replay_buffer.append((state_stack, action, reward, next_state_stack, done))\n",
        "        state_stack = next_state_stack\n",
        "        episode_reward += reward\n",
        "        frame_idx += 1\n",
        "\n",
        "        if len(replay_buffer) >= BATCH_SIZE:\n",
        "            states, actions, rewards, next_states, dones = sample_batch(replay_buffer, BATCH_SIZE, DEVICE)\n",
        "\n",
        "            q_values = policy_net(states).gather(1, actions.unsqueeze(1)).squeeze(1)\n",
        "            with torch.no_grad():\n",
        "                next_q = target_net(next_states).max(1)[0]\n",
        "                expected_q = rewards + gamma * next_q * (1 - dones)\n",
        "\n",
        "            loss = F.smooth_l1_loss(q_values, expected_q)\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        if frame_idx % TARGET_UPDATE_FREQ == 0:\n",
        "            target_net.load_state_dict(policy_net.state_dict())\n",
        "\n",
        "        if done:\n",
        "            state = preprocess(env.reset()[0])\n",
        "            state_stack = np.stack([state] * STACK_SIZE, axis=0)\n",
        "            episode_rewards.append(episode_reward)\n",
        "            print(f\"Frame {frame_idx}, Reward: {episode_reward:.2f}, Epsilon: {epsilon:.3f}\")\n",
        "            episode_reward = 0\n",
        "\n",
        "    avg_reward = np.mean(episode_rewards[-10:])\n",
        "    print(f\"[FINISHED] Avg reward over last 10 episodes: {avg_reward:.2f}\")\n",
        "    env.close()\n",
        "    torch.save(policy_net.state_dict(), \"temp_best_model.pth\")\n",
        "    return avg_reward"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "id": "5c335416",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== Training with LR=0.0001, FC=256, Gamma=0.95 ===\n",
            "Frame 852, Reward: -21.00, Epsilon: 0.999\n",
            "[FINISHED] Avg reward over last 10 episodes: -21.00\n",
            "\n",
            "=== Training with LR=0.0001, FC=256, Gamma=0.99 ===\n",
            "Frame 882, Reward: -21.00, Epsilon: 0.999\n",
            "[FINISHED] Avg reward over last 10 episodes: -21.00\n",
            "\n",
            "=== Training with LR=0.0001, FC=512, Gamma=0.95 ===\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/jollyjerr/code/@school/pong/venv/lib/python3.11/site-packages/numpy/_core/fromnumeric.py:3860: RuntimeWarning: Mean of empty slice.\n",
            "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
            "/Users/jollyjerr/code/@school/pong/venv/lib/python3.11/site-packages/numpy/_core/_methods.py:145: RuntimeWarning: invalid value encountered in scalar divide\n",
            "  ret = ret.dtype.type(ret / rcount)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[FINISHED] Avg reward over last 10 episodes: nan\n",
            "\n",
            "=== Training with LR=0.0001, FC=512, Gamma=0.99 ===\n",
            "[FINISHED] Avg reward over last 10 episodes: nan\n",
            "\n",
            "=== Training with LR=0.0005, FC=256, Gamma=0.95 ===\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[41]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[32m      6\u001b[39m best_model_state = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m hparams \u001b[38;5;129;01min\u001b[39;00m param_combos:\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m     avg_reward = \u001b[43mrun_training\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     10\u001b[39m     results[hparams] = avg_reward\n\u001b[32m     12\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m avg_reward > best_reward:\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[40]\u001b[39m\u001b[32m, line 51\u001b[39m, in \u001b[36mrun_training\u001b[39m\u001b[34m(hparams)\u001b[39m\n\u001b[32m     49\u001b[39m \u001b[38;5;66;03m# --- Learning step ---\u001b[39;00m\n\u001b[32m     50\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(replay_buffer) >= BATCH_SIZE:\n\u001b[32m---> \u001b[39m\u001b[32m51\u001b[39m     states, actions, rewards, next_states, dones = \u001b[43msample_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreplay_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mBATCH_SIZE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mDEVICE\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     53\u001b[39m     q_values = policy_net(states).gather(\u001b[32m1\u001b[39m, actions.unsqueeze(\u001b[32m1\u001b[39m)).squeeze(\u001b[32m1\u001b[39m)\n\u001b[32m     54\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[34]\u001b[39m\u001b[32m, line 14\u001b[39m, in \u001b[36msample_batch\u001b[39m\u001b[34m(buffer, batch_size, device)\u001b[39m\n\u001b[32m     10\u001b[39m batch = random.sample(buffer, batch_size)\n\u001b[32m     11\u001b[39m states, actions, rewards, next_states, dones = \u001b[38;5;28mzip\u001b[39m(*batch)\n\u001b[32m     13\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m     \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstates\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfloat32\u001b[49m\u001b[43m)\u001b[49m.to(device),\n\u001b[32m     15\u001b[39m     torch.tensor(np.array(actions), dtype=torch.int64).to(device),\n\u001b[32m     16\u001b[39m     torch.tensor(np.array(rewards), dtype=torch.float32).to(device),\n\u001b[32m     17\u001b[39m     torch.tensor(np.array(next_states), dtype=torch.float32).to(device),\n\u001b[32m     18\u001b[39m     torch.tensor(np.array(dones), dtype=torch.float32).to(device)\n\u001b[32m     19\u001b[39m )\n",
            "\u001b[31mKeyboardInterrupt\u001b[39m: "
          ]
        }
      ],
      "source": [
        "results = {}\n",
        "start_time = time.time()\n",
        "\n",
        "best_reward = float(\"-inf\")\n",
        "best_hparams = None\n",
        "best_model_state = None\n",
        "\n",
        "for hparams in param_combos:\n",
        "    avg_reward = run_training(hparams)\n",
        "    results[hparams] = avg_reward\n",
        "\n",
        "    if avg_reward > best_reward:\n",
        "        best_reward = avg_reward\n",
        "        best_hparams = hparams\n",
        "        best_model_state = torch.load(\"temp_best_model.pth\")\n",
        "\n",
        "print(\"\\n=== Grid Search Results ===\")\n",
        "for hparams, reward in sorted(results.items(), key=lambda x: -x[1]):\n",
        "    print(f\"LR={hparams[0]:.1e}, FC={hparams[1]}, Gamma={hparams[2]} --> Avg10 Reward: {reward:.2f}\")\n",
        "\n",
        "if best_model_state is not None:\n",
        "    torch.save(best_model_state, \"best_dqn_model.pth\")\n",
        "    print(f\"\\nBest model saved to 'best_dqn_model.pth' (LR={best_hparams[0]}, FC={best_hparams[1]}, Gamma={best_hparams[2]})\")\n",
        "\n",
        "print(f\"\\nTotal tuning time: {(time.time() - start_time)/60:.1f} min\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "saIwqiDRVdPM",
        "outputId": "8d2d1187-32cb-4cb4-b2ac-cf08058c2119"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== Training with LR=0.0001, FC=256, Gamma=0.95 ===\n",
            "Frame 1025, Reward: -19.00, Epsilon: 0.999\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-8-49cce46e6d05>:32: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:254.)\n",
            "  state_tensor = torch.tensor([state_stack], dtype=torch.float32).to(DEVICE)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Frame 1969, Reward: -21.00, Epsilon: 0.998\n",
            "Frame 2761, Reward: -21.00, Epsilon: 0.998\n",
            "Frame 3961, Reward: -18.00, Epsilon: 0.996\n",
            "Frame 4948, Reward: -20.00, Epsilon: 0.996\n",
            "Frame 5868, Reward: -19.00, Epsilon: 0.995\n",
            "Frame 6753, Reward: -21.00, Epsilon: 0.994\n",
            "Frame 7663, Reward: -21.00, Epsilon: 0.993\n",
            "Frame 8446, Reward: -21.00, Epsilon: 0.992\n",
            "Frame 9470, Reward: -19.00, Epsilon: 0.991\n",
            "Frame 10295, Reward: -21.00, Epsilon: 0.991\n",
            "Frame 11087, Reward: -21.00, Epsilon: 0.990\n",
            "Frame 12276, Reward: -19.00, Epsilon: 0.989\n",
            "Frame 13254, Reward: -21.00, Epsilon: 0.988\n",
            "Frame 14108, Reward: -21.00, Epsilon: 0.987\n",
            "Frame 14933, Reward: -21.00, Epsilon: 0.987\n",
            "Frame 15845, Reward: -21.00, Epsilon: 0.986\n",
            "Frame 16897, Reward: -19.00, Epsilon: 0.985\n",
            "Frame 17764, Reward: -20.00, Epsilon: 0.984\n",
            "Frame 18652, Reward: -20.00, Epsilon: 0.983\n",
            "Frame 19573, Reward: -19.00, Epsilon: 0.982\n",
            "Frame 20337, Reward: -21.00, Epsilon: 0.982\n",
            "Frame 21313, Reward: -21.00, Epsilon: 0.981\n",
            "Frame 22134, Reward: -21.00, Epsilon: 0.980\n",
            "Frame 22898, Reward: -21.00, Epsilon: 0.979\n",
            "Frame 23724, Reward: -21.00, Epsilon: 0.979\n",
            "Frame 24488, Reward: -21.00, Epsilon: 0.978\n",
            "Frame 25359, Reward: -20.00, Epsilon: 0.977\n",
            "Frame 26447, Reward: -18.00, Epsilon: 0.976\n",
            "Frame 27230, Reward: -21.00, Epsilon: 0.975\n",
            "Frame 28052, Reward: -21.00, Epsilon: 0.975\n",
            "Frame 29031, Reward: -20.00, Epsilon: 0.974\n",
            "Frame 30110, Reward: -20.00, Epsilon: 0.973\n",
            "Frame 31146, Reward: -19.00, Epsilon: 0.972\n",
            "Frame 32017, Reward: -20.00, Epsilon: 0.971\n",
            "Frame 33058, Reward: -20.00, Epsilon: 0.970\n",
            "Frame 34000, Reward: -21.00, Epsilon: 0.969\n",
            "Frame 34843, Reward: -21.00, Epsilon: 0.969\n",
            "Frame 35727, Reward: -20.00, Epsilon: 0.968\n",
            "Frame 36667, Reward: -21.00, Epsilon: 0.967\n",
            "Frame 37553, Reward: -21.00, Epsilon: 0.966\n",
            "Frame 38490, Reward: -19.00, Epsilon: 0.965\n",
            "Frame 39470, Reward: -20.00, Epsilon: 0.964\n",
            "Frame 40416, Reward: -21.00, Epsilon: 0.964\n",
            "Frame 41227, Reward: -21.00, Epsilon: 0.963\n",
            "Frame 42130, Reward: -20.00, Epsilon: 0.962\n",
            "Frame 42894, Reward: -21.00, Epsilon: 0.961\n",
            "Frame 43840, Reward: -21.00, Epsilon: 0.961\n",
            "Frame 44748, Reward: -21.00, Epsilon: 0.960\n",
            "Frame 45938, Reward: -18.00, Epsilon: 0.959\n",
            "Frame 46731, Reward: -21.00, Epsilon: 0.958\n",
            "Frame 47602, Reward: -21.00, Epsilon: 0.957\n",
            "Frame 48426, Reward: -21.00, Epsilon: 0.956\n",
            "Frame 49250, Reward: -21.00, Epsilon: 0.956\n",
            "Frame 50148, Reward: -21.00, Epsilon: 0.955\n",
            "Frame 51205, Reward: -19.00, Epsilon: 0.954\n",
            "Frame 52055, Reward: -21.00, Epsilon: 0.953\n",
            "Frame 53240, Reward: -19.00, Epsilon: 0.952\n",
            "Frame 54181, Reward: -21.00, Epsilon: 0.951\n",
            "Frame 55153, Reward: -19.00, Epsilon: 0.950\n",
            "Frame 56024, Reward: -20.00, Epsilon: 0.950\n",
            "Frame 56955, Reward: -20.00, Epsilon: 0.949\n",
            "Frame 57858, Reward: -20.00, Epsilon: 0.948\n",
            "Frame 58678, Reward: -21.00, Epsilon: 0.947\n",
            "Frame 59816, Reward: -20.00, Epsilon: 0.946\n",
            "Frame 60599, Reward: -21.00, Epsilon: 0.945\n",
            "Frame 61511, Reward: -21.00, Epsilon: 0.945\n",
            "Frame 62411, Reward: -20.00, Epsilon: 0.944\n",
            "Frame 63431, Reward: -20.00, Epsilon: 0.943\n",
            "Frame 64484, Reward: -19.00, Epsilon: 0.942\n",
            "Frame 65682, Reward: -20.00, Epsilon: 0.941\n",
            "Frame 66654, Reward: -21.00, Epsilon: 0.940\n",
            "Frame 67536, Reward: -21.00, Epsilon: 0.939\n",
            "Frame 68564, Reward: -20.00, Epsilon: 0.938\n",
            "Frame 69639, Reward: -19.00, Epsilon: 0.937\n",
            "Frame 70555, Reward: -21.00, Epsilon: 0.937\n",
            "Frame 71403, Reward: -21.00, Epsilon: 0.936\n",
            "Frame 72504, Reward: -19.00, Epsilon: 0.935\n",
            "Frame 73403, Reward: -21.00, Epsilon: 0.934\n",
            "Frame 74352, Reward: -20.00, Epsilon: 0.933\n",
            "Frame 75410, Reward: -20.00, Epsilon: 0.932\n",
            "Frame 76174, Reward: -21.00, Epsilon: 0.931\n",
            "Frame 77254, Reward: -20.00, Epsilon: 0.930\n",
            "Frame 78065, Reward: -21.00, Epsilon: 0.930\n",
            "Frame 79027, Reward: -20.00, Epsilon: 0.929\n",
            "Frame 79932, Reward: -20.00, Epsilon: 0.928\n",
            "Frame 80960, Reward: -20.00, Epsilon: 0.927\n",
            "Frame 81922, Reward: -21.00, Epsilon: 0.926\n",
            "Frame 82860, Reward: -21.00, Epsilon: 0.925\n",
            "Frame 83684, Reward: -21.00, Epsilon: 0.925\n",
            "Frame 84538, Reward: -21.00, Epsilon: 0.924\n",
            "Frame 85484, Reward: -21.00, Epsilon: 0.923\n",
            "Frame 86580, Reward: -21.00, Epsilon: 0.922\n",
            "Frame 87734, Reward: -19.00, Epsilon: 0.921\n",
            "Frame 88752, Reward: -20.00, Epsilon: 0.920\n",
            "Frame 89786, Reward: -20.00, Epsilon: 0.919\n",
            "Frame 90913, Reward: -19.00, Epsilon: 0.918\n",
            "Frame 91735, Reward: -21.00, Epsilon: 0.917\n",
            "Frame 92680, Reward: -21.00, Epsilon: 0.917\n",
            "Frame 93751, Reward: -19.00, Epsilon: 0.916\n",
            "Frame 94544, Reward: -21.00, Epsilon: 0.915\n",
            "Frame 95396, Reward: -21.00, Epsilon: 0.914\n",
            "Frame 96397, Reward: -19.00, Epsilon: 0.913\n",
            "Frame 97282, Reward: -21.00, Epsilon: 0.912\n",
            "Frame 98126, Reward: -21.00, Epsilon: 0.912\n",
            "Frame 99196, Reward: -19.00, Epsilon: 0.911\n",
            "Frame 100204, Reward: -19.00, Epsilon: 0.910\n",
            "Frame 101344, Reward: -20.00, Epsilon: 0.909\n",
            "Frame 102314, Reward: -21.00, Epsilon: 0.908\n",
            "Frame 103449, Reward: -19.00, Epsilon: 0.907\n",
            "Frame 104427, Reward: -20.00, Epsilon: 0.906\n",
            "Frame 105447, Reward: -21.00, Epsilon: 0.905\n",
            "Frame 106319, Reward: -21.00, Epsilon: 0.904\n",
            "Frame 107460, Reward: -21.00, Epsilon: 0.903\n",
            "Frame 108450, Reward: -20.00, Epsilon: 0.902\n",
            "Frame 109367, Reward: -20.00, Epsilon: 0.902\n",
            "Frame 110404, Reward: -19.00, Epsilon: 0.901\n",
            "Frame 111516, Reward: -20.00, Epsilon: 0.900\n",
            "Frame 112598, Reward: -21.00, Epsilon: 0.899\n",
            "Frame 113791, Reward: -20.00, Epsilon: 0.898\n",
            "Frame 114769, Reward: -20.00, Epsilon: 0.897\n",
            "Frame 115708, Reward: -20.00, Epsilon: 0.896\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-191d93883f2b>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mhparams\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mparam_combos\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mavg_reward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_training\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mhparams\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mavg_reward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-8-49cce46e6d05>\u001b[0m in \u001b[0;36mrun_training\u001b[0;34m(hparams)\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0;31m# --- Learning step ---\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreplay_buffer\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m             \u001b[0mstates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrewards\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdones\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msample_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreplay_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDEVICE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0mq_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpolicy_net\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgather\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-4-cbba06b4a2a1>\u001b[0m in \u001b[0;36msample_batch\u001b[0;34m(buffer, batch_size, device)\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrewards\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdones\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     )\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "results = {}\n",
        "start_time = time.time()\n",
        "\n",
        "best_reward = float(\"-inf\")\n",
        "best_hparams = None\n",
        "best_model_state = None\n",
        "\n",
        "for hparams in param_combos:\n",
        "    avg_reward = run_training(hparams)\n",
        "    results[hparams] = avg_reward\n",
        "\n",
        "    if avg_reward > best_reward:\n",
        "        best_reward = avg_reward\n",
        "        best_hparams = hparams\n",
        "        best_model_state = torch.load(\"temp_best_model.pth\")\n",
        "\n",
        "print(\"\\n=== Grid Search Results ===\")\n",
        "for hparams, reward in sorted(results.items(), key=lambda x: -x[1]):\n",
        "    print(f\"LR={hparams[0]:.1e}, FC={hparams[1]}, Gamma={hparams[2]} --> Avg10 Reward: {reward:.2f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jd3ICzBjRuji",
        "outputId": "49c6b156-4cf4-4612-f44a-7d526175cab9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2.6.0+cu124\n",
            "2.0.2\n"
          ]
        }
      ],
      "source": [
        "print(torch.__version__)\n",
        "print(np.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "VozWdQ8VT183"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import gymnasium as gym\n",
        "import numpy as np\n",
        "import random\n",
        "import cv2\n",
        "import time\n",
        "from collections import deque\n",
        "import ale_py\n",
        "\n",
        "gym.register_envs(ale_py)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "jMqBp9VgU6DI"
      },
      "outputs": [],
      "source": [
        "class DQN(nn.Module):\n",
        "     def __init__(self, input_shape, n_actions):\n",
        "        super(DQN, self).__init__()\n",
        "\n",
        "        self.conv1 = nn.Conv2d(input_shape[0], 32, kernel_size=8, stride=4)\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=4, stride=2)\n",
        "        self.conv3 = nn.Conv2d(64, 64, kernel_size=3, stride=1)\n",
        "\n",
        "        conv_w = self._conv_output_size(input_shape[1], 8, 4)\n",
        "        conv_w = self._conv_output_size(conv_w, 4, 2)\n",
        "        conv_w = self._conv_output_size(conv_w, 3, 1)\n",
        "\n",
        "        conv_h = self._conv_output_size(input_shape[2], 8, 4)\n",
        "        conv_h = self._conv_output_size(conv_h, 4, 2)\n",
        "        conv_h = self._conv_output_size(conv_h, 3, 1)\n",
        "\n",
        "        linear_input_size = conv_w * conv_h * 64\n",
        "\n",
        "        self.fc1 = nn.Linear(linear_input_size, 512)\n",
        "        self.fc2 = nn.Linear(512, n_actions)\n",
        "\n",
        "        self._initialize_weights()\n",
        "\n",
        "    def _conv_output_size(self, size, kernel_size, stride):\n",
        "        return (size - kernel_size) // stride + 1\n",
        "\n",
        "    def _initialize_weights(self):\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
        "                if m.bias is not None:\n",
        "                    nn.init.constant_(m.bias, 0)\n",
        "            elif isinstance(m, nn.Linear):\n",
        "                nn.init.kaiming_normal_(m.weight)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = F.relu(self.conv2(x))\n",
        "        x = F.relu(self.conv3(x))\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        return self.fc2(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "3nD2eBbdx7wo"
      },
      "outputs": [],
      "source": [
        "class ReplayBuffer:\n",
        "    def __init__(self, capacity):\n",
        "        self.buffer = deque(maxlen=capacity)\n",
        "\n",
        "    def push(self, state, action, reward, next_state, done):\n",
        "        self.buffer.append((state, action, reward, next_state, done))\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        batch = random.sample(self.buffer, batch_size)\n",
        "        state, action, reward, next_state, done = map(np.stack, zip(*batch))\n",
        "        return state, action, reward, next_state, done\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.buffer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "ffIvG7FHyA6e"
      },
      "outputs": [],
      "source": [
        "def preprocess_frame(frame):\n",
        "    \"\"\"Preprocess frame: grayscale, resize, normalize\"\"\"\n",
        "    gray = cv2.cvtColor(frame, cv2.COLOR_RGB2GRAY)\n",
        "    resized = cv2.resize(gray, (84, 84), interpolation=cv2.INTER_AREA)\n",
        "    normalized = resized.astype(np.float32) / 255.0\n",
        "    return normalized\n",
        "\n",
        "def stack_frames(stacked_frames, frame, is_new_episode=False):\n",
        "    frame = preprocess_frame(frame)\n",
        "\n",
        "    if is_new_episode:\n",
        "        stacked_frames = deque([frame for _ in range(4)], maxlen=4)\n",
        "    else:\n",
        "        stacked_frames.append(frame)\n",
        "\n",
        "    stacked_state = np.stack(stacked_frames, axis=0)\n",
        "    return stacked_state, stacked_frames\n",
        "\n",
        "def epsilon_greedy_policy(q_values, epsilon):\n",
        "    if random.random() < epsilon:\n",
        "        return random.randint(0, len(q_values) - 1)\n",
        "    else:\n",
        "        return np.argmax(q_values)\n",
        "\n",
        "def compute_td_loss(policy_net, target_net, replay_buffer, batch_size, gamma, device):\n",
        "    if len(replay_buffer) < batch_size:\n",
        "        return None\n",
        "\n",
        "    state, action, reward, next_state, done = replay_buffer.sample(batch_size)\n",
        "\n",
        "    state = torch.FloatTensor(state).to(device)\n",
        "    action = torch.LongTensor(action).to(device)\n",
        "    reward = torch.FloatTensor(reward).to(device)\n",
        "    next_state = torch.FloatTensor(next_state).to(device)\n",
        "    done = torch.BoolTensor(done).to(device)\n",
        "\n",
        "    q_values = policy_net(state)\n",
        "    q_value = q_values.gather(1, action.unsqueeze(1)).squeeze(1)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        next_q_values = target_net(next_state)\n",
        "        next_q_value = next_q_values.max(1)[0]\n",
        "        expected_q_value = reward + (gamma * next_q_value * ~done)\n",
        "\n",
        "    loss = F.smooth_l1_loss(q_value, expected_q_value)\n",
        "    return loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "BzAEwlmUyQYK"
      },
      "outputs": [],
      "source": [
        "def train_dqn():\n",
        "    # Hyperparameters\n",
        "    LEARNING_RATE = 1e-4\n",
        "    GAMMA = 0.99\n",
        "    EPSILON_START = 1.0\n",
        "    EPSILON_END = 0.01\n",
        "    EPSILON_DECAY = 1000000\n",
        "    BATCH_SIZE = 16          # Reduced from 32\n",
        "    TARGET_UPDATE = 10000\n",
        "    MEMORY_SIZE = 25000      # Reduced from 100000\n",
        "    MIN_MEMORY = 10000       # Reduced from 50000\n",
        "    MAX_EPISODES = 10000\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(f\"Using device: {device}\")\n",
        "\n",
        "    env = gym.make('ALE/Pong-v5', render_mode=None)\n",
        "    n_actions = env.action_space.n\n",
        "\n",
        "    policy_net = DQN((4, 84, 84), n_actions).to(device)\n",
        "    target_net = DQN((4, 84, 84), n_actions).to(device)\n",
        "    target_net.load_state_dict(policy_net.state_dict())\n",
        "    target_net.eval()\n",
        "\n",
        "    optimizer = optim.Adam(policy_net.parameters(), lr=LEARNING_RATE)\n",
        "    replay_buffer = ReplayBuffer(MEMORY_SIZE)\n",
        "\n",
        "    episode_rewards = []\n",
        "    losses = []\n",
        "    frame_count = 0\n",
        "    best_reward = float('-inf')\n",
        "\n",
        "    print(\"Starting training...\")\n",
        "    start_time = time.time()\n",
        "\n",
        "    for episode in range(MAX_EPISODES):\n",
        "        obs, _ = env.reset()\n",
        "        stacked_frames = None\n",
        "        state, stacked_frames = stack_frames(stacked_frames, obs, is_new_episode=True)\n",
        "\n",
        "        episode_reward = 0\n",
        "        episode_loss = []\n",
        "\n",
        "        while True:\n",
        "            frame_count += 1\n",
        "\n",
        "            epsilon = EPSILON_END + (EPSILON_START - EPSILON_END) * \\\n",
        "                     np.exp(-1.0 * frame_count / EPSILON_DECAY)\n",
        "\n",
        "            with torch.no_grad():\n",
        "                state_tensor = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
        "                q_values = policy_net(state_tensor).cpu().numpy()[0]\n",
        "\n",
        "            action = epsilon_greedy_policy(q_values, epsilon)\n",
        "\n",
        "            next_obs, reward, terminated, truncated, info = env.step(action)\n",
        "            done = terminated or truncated\n",
        "\n",
        "            reward = np.clip(reward, -1, 1)\n",
        "\n",
        "            next_state, stacked_frames = stack_frames(stacked_frames, next_obs, is_new_episode=False)\n",
        "\n",
        "            replay_buffer.push(state, action, reward, next_state, done)\n",
        "\n",
        "            state = next_state\n",
        "            episode_reward += reward\n",
        "\n",
        "            if len(replay_buffer) >= MIN_MEMORY and frame_count % 4 == 0:\n",
        "                loss = compute_td_loss(policy_net, target_net, replay_buffer,\n",
        "                                     BATCH_SIZE, GAMMA, device)\n",
        "                if loss is not None:\n",
        "                    optimizer.zero_grad()\n",
        "                    loss.backward()\n",
        "                    torch.nn.utils.clip_grad_norm_(policy_net.parameters(), 10)\n",
        "                    optimizer.step()\n",
        "\n",
        "                    losses.append(loss.item())\n",
        "                    episode_loss.append(loss.item())\n",
        "\n",
        "            if frame_count % TARGET_UPDATE == 0:\n",
        "                target_net.load_state_dict(policy_net.state_dict())\n",
        "                print(f\"Updated target network at frame {frame_count}\")\n",
        "\n",
        "            if done:\n",
        "                break\n",
        "\n",
        "        episode_rewards.append(episode_reward)\n",
        "\n",
        "        if episode % 100 == 0:\n",
        "            avg_reward = np.mean(episode_rewards[-100:])\n",
        "            avg_loss = np.mean(losses[-1000:]) if len(losses) >= 1000 else np.mean(losses) if losses else 0\n",
        "\n",
        "            elapsed_time = time.time() - start_time\n",
        "            print(f\"Episode {episode}\")\n",
        "            print(f\"  Average Reward (last 100): {avg_reward:.2f}\")\n",
        "            print(f\"  Current Epsilon: {epsilon:.3f}\")\n",
        "            print(f\"  Average Loss: {avg_loss:.4f}\")\n",
        "            print(f\"  Frame Count: {frame_count}\")\n",
        "            print(f\"  Elapsed Time: {elapsed_time/60:.1f} min\")\n",
        "            print(f\"  Replay Buffer Size: {len(replay_buffer)}\")\n",
        "            print(\"-\" * 50)\n",
        "\n",
        "            if avg_reward > best_reward:\n",
        "                best_reward = avg_reward\n",
        "                torch.save({\n",
        "                    'policy_net_state_dict': policy_net.state_dict(),\n",
        "                    'target_net_state_dict': target_net.state_dict(),\n",
        "                    'optimizer_state_dict': optimizer.state_dict(),\n",
        "                    'episode': episode,\n",
        "                    'best_reward': best_reward,\n",
        "                    'frame_count': frame_count\n",
        "                }, 'best_dqn_pong.pth')\n",
        "                print(f\"New best model saved! Average reward: {best_reward:.2f}\")\n",
        "\n",
        "        if len(episode_rewards) >= 100 and np.mean(episode_rewards[-100:]) >= 18.0:\n",
        "            print(f\"Solved! Average reward over last 100 episodes: {np.mean(episode_rewards[-100:]):.2f}\")\n",
        "            break\n",
        "\n",
        "    env.close()\n",
        "    print(\"Training completed!\")\n",
        "\n",
        "    return episode_rewards"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "VkElrVhAydRV",
        "outputId": "10e4705f-0783-4547-dad8-9646b76b3bc7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting DQN training for Pong...\n",
            "Using device: cuda\n",
            "Starting training...\n",
            "Episode 0\n",
            "  Average Reward (last 100): -20.00\n",
            "  Current Epsilon: 0.999\n",
            "  Average Loss: 0.0000\n",
            "  Frame Count: 966\n",
            "  Elapsed Time: 0.1 min\n",
            "  Replay Buffer Size: 966\n",
            "--------------------------------------------------\n",
            "New best model saved! Average reward: -20.00\n",
            "Updated target network at frame 10000\n",
            "Updated target network at frame 20000\n",
            "Updated target network at frame 30000\n",
            "Updated target network at frame 40000\n",
            "Updated target network at frame 50000\n",
            "Updated target network at frame 60000\n",
            "Updated target network at frame 70000\n",
            "Updated target network at frame 80000\n",
            "Updated target network at frame 90000\n",
            "Episode 100\n",
            "  Average Reward (last 100): -20.33\n",
            "  Current Epsilon: 0.912\n",
            "  Average Loss: 0.0032\n",
            "  Frame Count: 93358\n",
            "  Elapsed Time: 4.8 min\n",
            "  Replay Buffer Size: 25000\n",
            "--------------------------------------------------\n",
            "Updated target network at frame 100000\n",
            "Updated target network at frame 110000\n",
            "Updated target network at frame 120000\n",
            "Updated target network at frame 130000\n",
            "Updated target network at frame 140000\n",
            "Updated target network at frame 150000\n",
            "Updated target network at frame 160000\n",
            "Updated target network at frame 170000\n",
            "Updated target network at frame 180000\n",
            "Episode 200\n",
            "  Average Reward (last 100): -20.22\n",
            "  Current Epsilon: 0.829\n",
            "  Average Loss: 0.0026\n",
            "  Frame Count: 189276\n",
            "  Elapsed Time: 10.0 min\n",
            "  Replay Buffer Size: 25000\n",
            "--------------------------------------------------\n",
            "Updated target network at frame 190000\n",
            "Updated target network at frame 200000\n",
            "Updated target network at frame 210000\n",
            "Updated target network at frame 220000\n",
            "Updated target network at frame 230000\n",
            "Updated target network at frame 240000\n",
            "Updated target network at frame 250000\n",
            "Updated target network at frame 260000\n",
            "Updated target network at frame 270000\n",
            "Updated target network at frame 280000\n",
            "Updated target network at frame 290000\n",
            "Episode 300\n",
            "  Average Reward (last 100): -19.96\n",
            "  Current Epsilon: 0.747\n",
            "  Average Loss: 0.0049\n",
            "  Frame Count: 294852\n",
            "  Elapsed Time: 15.6 min\n",
            "  Replay Buffer Size: 25000\n",
            "--------------------------------------------------\n",
            "New best model saved! Average reward: -19.96\n",
            "Updated target network at frame 300000\n",
            "Updated target network at frame 310000\n",
            "Updated target network at frame 320000\n",
            "Updated target network at frame 330000\n",
            "Updated target network at frame 340000\n",
            "Updated target network at frame 350000\n",
            "Updated target network at frame 360000\n",
            "Updated target network at frame 370000\n",
            "Updated target network at frame 380000\n",
            "Updated target network at frame 390000\n",
            "Updated target network at frame 400000\n",
            "Updated target network at frame 410000\n",
            "Episode 400\n",
            "  Average Reward (last 100): -19.35\n",
            "  Current Epsilon: 0.662\n",
            "  Average Loss: 0.0037\n",
            "  Frame Count: 417246\n",
            "  Elapsed Time: 22.1 min\n",
            "  Replay Buffer Size: 25000\n",
            "--------------------------------------------------\n",
            "New best model saved! Average reward: -19.35\n",
            "Updated target network at frame 420000\n",
            "Updated target network at frame 430000\n",
            "Updated target network at frame 440000\n",
            "Updated target network at frame 450000\n",
            "Updated target network at frame 460000\n",
            "Updated target network at frame 470000\n",
            "Updated target network at frame 480000\n",
            "Updated target network at frame 490000\n",
            "Updated target network at frame 500000\n",
            "Updated target network at frame 510000\n",
            "Updated target network at frame 520000\n",
            "Updated target network at frame 530000\n",
            "Updated target network at frame 540000\n",
            "Updated target network at frame 550000\n",
            "Episode 500\n",
            "  Average Reward (last 100): -18.58\n",
            "  Current Epsilon: 0.578\n",
            "  Average Loss: 0.0040\n",
            "  Frame Count: 555302\n",
            "  Elapsed Time: 29.4 min\n",
            "  Replay Buffer Size: 25000\n",
            "--------------------------------------------------\n",
            "New best model saved! Average reward: -18.58\n",
            "Updated target network at frame 560000\n",
            "Updated target network at frame 570000\n",
            "Updated target network at frame 580000\n",
            "Updated target network at frame 590000\n",
            "Updated target network at frame 600000\n",
            "Updated target network at frame 610000\n",
            "Updated target network at frame 620000\n",
            "Updated target network at frame 630000\n",
            "Updated target network at frame 640000\n",
            "Updated target network at frame 650000\n",
            "Updated target network at frame 660000\n",
            "Updated target network at frame 670000\n",
            "Updated target network at frame 680000\n",
            "Updated target network at frame 690000\n",
            "Updated target network at frame 700000\n",
            "Episode 600\n",
            "  Average Reward (last 100): -18.41\n",
            "  Current Epsilon: 0.498\n",
            "  Average Loss: 0.0036\n",
            "  Frame Count: 706662\n",
            "  Elapsed Time: 37.3 min\n",
            "  Replay Buffer Size: 25000\n",
            "--------------------------------------------------\n",
            "New best model saved! Average reward: -18.41\n",
            "Updated target network at frame 710000\n",
            "Updated target network at frame 720000\n",
            "Updated target network at frame 730000\n",
            "Updated target network at frame 740000\n",
            "Updated target network at frame 750000\n",
            "Updated target network at frame 760000\n",
            "Updated target network at frame 770000\n",
            "Updated target network at frame 780000\n",
            "Updated target network at frame 790000\n",
            "Updated target network at frame 800000\n",
            "Updated target network at frame 810000\n",
            "Updated target network at frame 820000\n",
            "Updated target network at frame 830000\n",
            "Updated target network at frame 840000\n",
            "Updated target network at frame 850000\n",
            "Updated target network at frame 860000\n",
            "Updated target network at frame 870000\n",
            "Updated target network at frame 880000\n",
            "Episode 700\n",
            "  Average Reward (last 100): -17.40\n",
            "  Current Epsilon: 0.420\n",
            "  Average Loss: 0.0041\n",
            "  Frame Count: 881190\n",
            "  Elapsed Time: 46.4 min\n",
            "  Replay Buffer Size: 25000\n",
            "--------------------------------------------------\n",
            "New best model saved! Average reward: -17.40\n",
            "Updated target network at frame 890000\n",
            "Updated target network at frame 900000\n",
            "Updated target network at frame 910000\n",
            "Updated target network at frame 920000\n",
            "Updated target network at frame 930000\n",
            "Updated target network at frame 940000\n",
            "Updated target network at frame 950000\n",
            "Updated target network at frame 960000\n",
            "Updated target network at frame 970000\n",
            "Updated target network at frame 980000\n",
            "Updated target network at frame 990000\n",
            "Updated target network at frame 1000000\n",
            "Updated target network at frame 1010000\n",
            "Updated target network at frame 1020000\n",
            "Updated target network at frame 1030000\n",
            "Updated target network at frame 1040000\n",
            "Updated target network at frame 1050000\n",
            "Updated target network at frame 1060000\n",
            "Updated target network at frame 1070000\n",
            "Updated target network at frame 1080000\n",
            "Episode 800\n",
            "  Average Reward (last 100): -16.52\n",
            "  Current Epsilon: 0.345\n",
            "  Average Loss: 0.0039\n",
            "  Frame Count: 1082335\n",
            "  Elapsed Time: 56.8 min\n",
            "  Replay Buffer Size: 25000\n",
            "--------------------------------------------------\n",
            "New best model saved! Average reward: -16.52\n",
            "Updated target network at frame 1090000\n",
            "Updated target network at frame 1100000\n",
            "Updated target network at frame 1110000\n",
            "Updated target network at frame 1120000\n",
            "Updated target network at frame 1130000\n",
            "Updated target network at frame 1140000\n",
            "Updated target network at frame 1150000\n",
            "Updated target network at frame 1160000\n",
            "Updated target network at frame 1170000\n",
            "Updated target network at frame 1180000\n",
            "Updated target network at frame 1190000\n",
            "Updated target network at frame 1200000\n",
            "Updated target network at frame 1210000\n",
            "Updated target network at frame 1220000\n",
            "Updated target network at frame 1230000\n",
            "Updated target network at frame 1240000\n",
            "Updated target network at frame 1250000\n",
            "Updated target network at frame 1260000\n",
            "Updated target network at frame 1270000\n",
            "Updated target network at frame 1280000\n",
            "Updated target network at frame 1290000\n",
            "Updated target network at frame 1300000\n",
            "Updated target network at frame 1310000\n",
            "Episode 900\n",
            "  Average Reward (last 100): -15.46\n",
            "  Current Epsilon: 0.276\n",
            "  Average Loss: 0.0040\n",
            "  Frame Count: 1312552\n",
            "  Elapsed Time: 68.6 min\n",
            "  Replay Buffer Size: 25000\n",
            "--------------------------------------------------\n",
            "New best model saved! Average reward: -15.46\n",
            "Updated target network at frame 1320000\n",
            "Updated target network at frame 1330000\n",
            "Updated target network at frame 1340000\n",
            "Updated target network at frame 1350000\n",
            "Updated target network at frame 1360000\n",
            "Updated target network at frame 1370000\n",
            "Updated target network at frame 1380000\n",
            "Updated target network at frame 1390000\n",
            "Updated target network at frame 1400000\n",
            "Updated target network at frame 1410000\n",
            "Updated target network at frame 1420000\n",
            "Updated target network at frame 1430000\n",
            "Updated target network at frame 1440000\n",
            "Updated target network at frame 1450000\n",
            "Updated target network at frame 1460000\n",
            "Updated target network at frame 1470000\n",
            "Updated target network at frame 1480000\n",
            "Updated target network at frame 1490000\n",
            "Updated target network at frame 1500000\n",
            "Updated target network at frame 1510000\n",
            "Updated target network at frame 1520000\n",
            "Updated target network at frame 1530000\n",
            "Updated target network at frame 1540000\n",
            "Updated target network at frame 1550000\n",
            "Updated target network at frame 1560000\n",
            "Updated target network at frame 1570000\n",
            "Episode 1000\n",
            "  Average Reward (last 100): -13.26\n",
            "  Current Epsilon: 0.215\n",
            "  Average Loss: 0.0037\n",
            "  Frame Count: 1575710\n",
            "  Elapsed Time: 82.5 min\n",
            "  Replay Buffer Size: 25000\n",
            "--------------------------------------------------\n",
            "New best model saved! Average reward: -13.26\n",
            "Updated target network at frame 1580000\n",
            "Updated target network at frame 1590000\n",
            "Updated target network at frame 1600000\n",
            "Updated target network at frame 1610000\n",
            "Updated target network at frame 1620000\n",
            "Updated target network at frame 1630000\n",
            "Updated target network at frame 1640000\n",
            "Updated target network at frame 1650000\n",
            "Updated target network at frame 1660000\n",
            "Updated target network at frame 1670000\n",
            "Updated target network at frame 1680000\n",
            "Updated target network at frame 1690000\n",
            "Updated target network at frame 1700000\n",
            "Updated target network at frame 1710000\n",
            "Updated target network at frame 1720000\n",
            "Updated target network at frame 1730000\n",
            "Updated target network at frame 1740000\n",
            "Updated target network at frame 1750000\n",
            "Updated target network at frame 1760000\n",
            "Updated target network at frame 1770000\n",
            "Updated target network at frame 1780000\n",
            "Updated target network at frame 1790000\n",
            "Updated target network at frame 1800000\n",
            "Updated target network at frame 1810000\n",
            "Updated target network at frame 1820000\n",
            "Updated target network at frame 1830000\n",
            "Updated target network at frame 1840000\n",
            "Updated target network at frame 1850000\n",
            "Updated target network at frame 1860000\n",
            "Updated target network at frame 1870000\n",
            "Episode 1100\n",
            "  Average Reward (last 100): -9.99\n",
            "  Current Epsilon: 0.162\n",
            "  Average Loss: 0.0047\n",
            "  Frame Count: 1873609\n",
            "  Elapsed Time: 98.3 min\n",
            "  Replay Buffer Size: 25000\n",
            "--------------------------------------------------\n",
            "New best model saved! Average reward: -9.99\n",
            "Updated target network at frame 1880000\n",
            "Updated target network at frame 1890000\n",
            "Updated target network at frame 1900000\n",
            "Updated target network at frame 1910000\n",
            "Updated target network at frame 1920000\n",
            "Updated target network at frame 1930000\n",
            "Updated target network at frame 1940000\n",
            "Updated target network at frame 1950000\n",
            "Updated target network at frame 1960000\n",
            "Updated target network at frame 1970000\n",
            "Updated target network at frame 1980000\n",
            "Updated target network at frame 1990000\n",
            "Updated target network at frame 2000000\n",
            "Updated target network at frame 2010000\n",
            "Updated target network at frame 2020000\n",
            "Updated target network at frame 2030000\n",
            "Updated target network at frame 2040000\n",
            "Updated target network at frame 2050000\n",
            "Updated target network at frame 2060000\n",
            "Updated target network at frame 2070000\n",
            "Updated target network at frame 2080000\n",
            "Updated target network at frame 2090000\n",
            "Updated target network at frame 2100000\n",
            "Updated target network at frame 2110000\n",
            "Updated target network at frame 2120000\n",
            "Updated target network at frame 2130000\n",
            "Updated target network at frame 2140000\n",
            "Updated target network at frame 2150000\n",
            "Updated target network at frame 2160000\n",
            "Updated target network at frame 2170000\n",
            "Updated target network at frame 2180000\n",
            "Updated target network at frame 2190000\n",
            "Updated target network at frame 2200000\n",
            "Episode 1200\n",
            "  Average Reward (last 100): -2.10\n",
            "  Current Epsilon: 0.119\n",
            "  Average Loss: 0.0041\n",
            "  Frame Count: 2202198\n",
            "  Elapsed Time: 115.7 min\n",
            "  Replay Buffer Size: 25000\n",
            "--------------------------------------------------\n",
            "New best model saved! Average reward: -2.10\n",
            "Updated target network at frame 2210000\n",
            "Updated target network at frame 2220000\n",
            "Updated target network at frame 2230000\n",
            "Updated target network at frame 2240000\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-0fda965ee012>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Train the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Starting DQN training for Pong...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mepisode_rewards\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_dqn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Test the trained model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-6-45aa103b24bb>\u001b[0m in \u001b[0;36mtrain_dqn\u001b[0;34m()\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m             \u001b[0;31m# Take action\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m             \u001b[0mnext_obs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mterminated\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtruncated\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m             \u001b[0mdone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mterminated\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mtruncated\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/gymnasium/wrappers/common.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    391\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_has_reset\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    392\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mResetNeeded\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Cannot call env.step() before calling env.reset()\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 393\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    394\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    395\u001b[0m     def reset(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/gymnasium/core.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    325\u001b[0m     ) -> tuple[WrapperObsType, SupportsFloat, bool, bool, dict[str, Any]]:\n\u001b[1;32m    326\u001b[0m         \u001b[0;34m\"\"\"Uses the :meth:`step` of the :attr:`env` that can be overwritten to change the returned data.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 327\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    328\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    329\u001b[0m     def reset(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/gymnasium/wrappers/common.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    283\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0menv_step_passive_checker\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    284\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 285\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    286\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    287\u001b[0m     def reset(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/ale_py/env.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    308\u001b[0m         \u001b[0mis_truncated\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0male\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgame_truncated\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 310\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_obs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_terminal\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_truncated\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    311\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    312\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/ale_py/env.py\u001b[0m in \u001b[0;36m_get_obs\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    327\u001b[0m             \u001b[0mimage_obs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0male\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetRAM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_obs_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"rgb\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 329\u001b[0;31m             \u001b[0mimage_obs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0male\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetScreenRGB\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    330\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_obs_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"grayscale\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    331\u001b[0m             \u001b[0mimage_obs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0male\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetScreenGrayscale\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "print(\"Starting DQN training for Pong...\")\n",
        "episode_rewards = train_dqn()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oGez8pm_zHgz"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
